{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Moana\",\n",
    "        \"answer\": \"\"\"\n",
    "        🌊👩💪\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"The Godfather\",\n",
    "        \"answer\": \"\"\"\n",
    "        👨‍👨‍👦🔫🍝\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Top Gun\",\n",
    "        \"answer\": \"\"\"\n",
    "        🛩️👨‍✈️🔥\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Home Alone\",\n",
    "        \"answer\": \"\"\"\n",
    "        👦🏠🎄\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"The Lion King\",\n",
    "        \"answer\": \"\"\"\n",
    "        🦁👑🌅\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "    human :{question},\n",
    "    You : {answer},\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_template,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "history_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an history between you and human:\n",
    "    {history}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "start_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "    DON'T ever confuse what was in the example with history.\n",
    "    When Human ask about the conversation record, You answer in English by referring to the conversation record, and when Human ask about the movie title, YouI answer with an emoji by referring to the example.\n",
    "    When you use emojis, you have to use ONLY three.\n",
    "    Human: {question}\n",
    "    You:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompts = [(\"example\", example_prompt), (\"history\", history_prompt), (\"start\", start_prompt)]\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {example}\n",
    "    Example Done!\n",
    "    ----------\n",
    "    {history}\n",
    "    History Done!\n",
    "    ----------\n",
    "    {start}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final_prompt,\n",
    "    pipeline_prompts=prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=500,\n",
    "    memory_key=\"history\",\n",
    ")\n",
    "\n",
    "\n",
    "def memory_save(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\" :  output})\n",
    "\n",
    "\n",
    "def memory_load(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "chain = (\n",
    "    {\"history\": RunnableLambda(memory_load), \"question\": RunnablePassthrough(),}\n",
    "    | full_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_chain(question):\n",
    "    result = chain.invoke(question).content\n",
    "    memory_save(question, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚢💔🌊🦸‍♂️🦸‍♀️🌍"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Titanic\")\n",
    "invoke_chain(\"The Avengers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first movie you asked about was \"Titanic.\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The first movie you asked about was \"Titanic.\"'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"In reference to conversation records, what was the first movie I asked?\").content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
