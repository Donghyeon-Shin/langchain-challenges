{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "def embed_file():\n",
    "    cache_dir = LocalFileStore(\"./.cache/\")\n",
    "    loader = UnstructuredFileLoader(\"./files/document.txt\")\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=50,\n",
    "    )\n",
    "    docs = loader.load_and_split(text_splitter=splitter)\n",
    "    embedder = OpenAIEmbeddings()\n",
    "    cache_embedder = CacheBackedEmbeddings.from_bytes_store(embedder, cache_dir)\n",
    "    vectorStore = FAISS.from_documents(docs, cache_embedder)\n",
    "    retriever = vectorStore.as_retriever()\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True,\n",
    "    memory_key=\"history\"\n",
    ")\n",
    "\n",
    "def save_memory(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\" :  output})\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_doc(document):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a helpful assistant. Answer questions using only the following context. and You remember conversations with human.\n",
    "            DON'T make it up.\n",
    "            --------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever = embed_file()\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(format_doc),\n",
    "        \"history\": RunnableLambda(load_memory),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke(question).content\n",
    "    save_memory(question, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, Winston believes that Jones, Aaronson, and Rutherford are guilty of the crimes they are charged with, and he has convinced himself that he had never seen the photograph that disproved their guilt, which he had invented. However, this belief is a product of the Party's manipulation and the process of crimestop, where he trains himself to accept the Party's version of reality without question."
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Is Aaronson guilty?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winston wrote the following messages on the slate: \n",
      "\n",
      "1. FREEDOM IS SLAVERY\n",
      "2. TWO AND TWO MAKE FIVE\n",
      "3. GOD IS POWER"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What message did he write in the table?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia is a character who is Winston's love interest. In the context, she represents a connection to love and rebellion against the oppressive regime of the Party. Winston experiences a deep emotional connection with her, and despite the dangers of their relationship, he feels a strong desire to be with her. Julia is also someone he believes needs his help, indicating a bond that goes beyond mere physical attraction."
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Who is Julia?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
